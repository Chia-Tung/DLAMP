# LightningDatamodule
input_len: 1
output_len: 1
split_config:
  ratios: [6, 2, 2]
  split_method: "sequential"
sampling_rate: 2
batch_size: 1
workers: 8 # mpi proc = 32 num_gpus = 8 max_user_processes = 2048
# LightningModule
surface_alpha: 0.25
optim_config:
  name: AdamW
  args:
    lr: 2e-4
    weight_decay: 3e-6
lr_schedule:
  name: cosine
  args:
    warmup_steps: 1000
# lightning.Trainer
num_gpus: null
strategy: "auto"
fast_dev_run: False
max_epochs: null
max_steps: null
min_steps: 5e4 # 50k
limit_train_batches: null
limit_val_batches: null
early_stop_patience: 10
log_image_every_n_steps: 5e3
log_every_n_steps: 50 # default 50
resume_from_checkpoint: null
precision: "bf16-mixed"